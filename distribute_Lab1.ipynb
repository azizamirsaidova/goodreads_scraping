{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWHZlEbr_34A"
   },
   "source": [
    "## Lab 1: Goodreads Parsing and Scraping\r\n",
    "**Univ.AI** <br>\r\n",
    "**DS-1 Cohort 1**\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dcNwcwJsJhh"
   },
   "source": [
    "## Table of Contents\r\n",
    "* [Lab1: Goodreads Scraping and Parsing](#Lab1:-Goodreads-Scraping-and-Parsing)\r\n",
    "  * [Learning Goals](##Learning-Goals)\r\n",
    "  * [Q1: Scrape the \"Best Books ever\" web page](##Q1:-Scrape-the-\"Best-Books-ever\"-web-page)\r\n",
    "  * [Q2: Parse the page, extract book urls](##Q2:-Parse-the-page,-extract-book-urls)\r\n",
    "  * [Q3: Scrape the web page of each book](##Q3:-Scrape-the-web-page-of-each-book)\r\n",
    "  * [Q4: Parse each books page, extract information](##Q4:-Parse-each-books-page,-extract-information)\r\n",
    "    * [4.1 Extract genres](###4.1-Extract-Genres)\r\n",
    "    * [4.2 Extract Published Year](###4.2-Extract-Published-Year)\r\n",
    "    * [4.3 Extract Rating, ISBN, Title of the book, Author and Rating Count](###4.3-Extract-Rating,-ISBN,-Title-of-the-book,-Author-and-Rating-Count)\r\n",
    "    * [4.4 Creating a DataFrame](###4.4-Creating-a-DataFrame)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3Tuwno9_367"
   },
   "source": [
    "## Learning Goals \n",
    "Goodreads has put out a list of the \"Best Books ever\", as voted on by around 200,000 people from the general Goodreads community. \n",
    "\n",
    "In this lab, we will be scraping and extracting information from Goodread's \"[Best Books ever](https://www.goodreads.com/list/show/1.Best_Books_Ever?page=1)\" list.\n",
    "\n",
    "This lab consists of four main parts:\n",
    "1. Scraping the \"Best Books ever\" web page\n",
    "2. Parsing the page, extract book urls\n",
    "3. Scraping the web page of each book\n",
    "4. Parsing a book page, extract book properties\n",
    "\n",
    "This lab will develop your skills in:\n",
    "* Exploring web pages through developer tools \n",
    "* Scraping and Parsing using Beautiful Soup and requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6PPYEYMj_37E"
   },
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import time, requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxSM_hL__37K"
   },
   "source": [
    "## Q1: Scrape the  \"Best Books ever\" web page\n",
    "We're going to see the structure of Goodread's best books list. \n",
    "\n",
    "To get this page we use pythons [requests module](https://requests.readthedocs.io/en/master/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yfZ5KA5i_37O"
   },
   "outputs": [],
   "source": [
    "#Getting the url using requests module\n",
    "URLSTART=\"https://www.goodreads.com\"\n",
    "BESTBOOKS=\"/list/show/1.Best_Books_Ever?page=\"\n",
    "url = URLSTART+BESTBOOKS+'1'\n",
    "page = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63XN7F7evAK5"
   },
   "source": [
    "Check the status of the page - 200 is OK and 404 is not good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DayARHGc_37d",
    "outputId": "2063b858-cbd9-4fa8-d7bf-38543650d534"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLc5msTFvIOt"
   },
   "source": [
    "Now that we are familiar with pythons request module, let us access the first two pages from Goodreads. \r\n",
    "\r\n",
    "This will mean you will need to scrape **two** URLs: https://www.goodreads.com/list/show/1.Best_Books_Ever?page=1 and  https://www.goodreads.com/list/show/1.Best_Books_Ever?page=2. \r\n",
    "\r\n",
    "<br>\r\n",
    "\r\n",
    "**Hint:** To do this, you can put your request.get() function in a `for` loop.<br>\r\n",
    "You will also need to use the time.sleep() function to wait for 1 second between the two get requests so that Goodread's doesn't think you are a threat attempting to mount a denial-of-service attack!<br>\r\n",
    "In addition to this, store the HTML text in a dictionary `page_dict`. The key should be the page number (1 or 2) and the value should be the HTML text corresponding the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CFAKLa5d_37i"
   },
   "outputs": [],
   "source": [
    "#Loop to fetch 2 pages of \"best-books\" from goodreads. \n",
    "URLSTART=\"https://www.goodreads.com\"\n",
    "BESTBOOKS=\"/list/show/1.Best_Books_Ever?page=\"\n",
    "\n",
    "page_dict={}\n",
    "\n",
    "#your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghXGaY1s_37p"
   },
   "source": [
    "## Q2: Parse the page, extract book urls\n",
    "\n",
    "Our next step should be to parse the HTML text we have saved from the previous section and extract the information we need. \n",
    "\n",
    "To do this, we will be using BeautifulSoup to transform HTML content into Python data structures. You can also use other libraries like PyQuery if you are comfortable with jQuery, but we will be using BeautifulSoup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "es2gh_EQ_37w"
   },
   "outputs": [],
   "source": [
    "#Import BeautifulSoup\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOeTQrYtxILB"
   },
   "source": [
    "Our aim is to extract the **book URLs** on the page in order to use it in further sections. \r\n",
    "\r\n",
    "To do this, we look for the elements with class bookTitle, extract the urls, and write them into a dictionary `urldict` where the keys are the page numbers and the values are a list of the book URLs extracted from the page.\r\n",
    "\r\n",
    "\r\n",
    "**Hint:** While parsing the HTML, look for the HTML a element, but only the one that has a CSS class of `bookTitle`. If you look at the page source, you'll see a construct like **`class=bookTitle`** on the table as seen below:\r\n",
    "<br><br>\r\n",
    "![goodreadsexample](https://drive.google.com/uc?export=view&id=1PUvIe7VkXFSkvrj4pZm0pRKWr6W8Rw0F)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "iAjmZFIH_375"
   },
   "outputs": [],
   "source": [
    "urldict={}\n",
    "\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5c9Ip_A_379"
   },
   "source": [
    "## Q3: Scrape the web page of each book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GB1tiwv4qdH"
   },
   "source": [
    "Now that we have the book URLs in a dictionary `urldict`, we can parse the web pages of the books itself to extract some information. <br>\r\n",
    "\r\n",
    "Before we extract information, we will need to get the HTML text for each book's page. \r\n",
    "Scrape the books web pages and store the HTML text for each book in a dictionary named `bookdict` using a `for` loop in a similar fashion as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Hfb0MK3_37_",
    "outputId": "49116482-db77-4f2a-dde6-5fdd2a49ae1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "#This is just an **example** to understand how to scarpe these files\n",
    "#Scraping one of the files\n",
    "URLSTART=\"https://www.goodreads.com\"\n",
    "book_url=URLSTART+urldict[2][0]\n",
    "stuff=requests.get(book_url)\n",
    "\n",
    "#Check the status of the page\n",
    "print(stuff.status_code)\n",
    "\n",
    "#All OK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hjUWYtrkBV9N"
   },
   "outputs": [],
   "source": [
    "#Fetching the actual 200 book pages\r\n",
    "#In the interest of time, we are taking just the first 10 of each page. Running this for 200 books takes 25 min!\r\n",
    "bookdict={}\r\n",
    "\r\n",
    "#your code here\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeDDwZn6-FSn"
   },
   "source": [
    "##Q4: Parse each books page, extract information\r\n",
    "\r\n",
    "Now that we have the HTML text for the books, we can extract information from these web pages. \r\n",
    "We intend to extract the following data:\r\n",
    "\r\n",
    "- Published year\r\n",
    "- Rating\r\n",
    "- ISBN \r\n",
    "- Title of the book\r\n",
    "- Author\r\n",
    "- Genres this book fits in. Since there are several genres associated with each book, you will need to extract the URL of each genre, separated by a pipe '|' like so:\r\n",
    "```\r\n",
    "/genres/young-adult|/genres/fiction|/genres/science-fiction|/genres/dystopia|/genres/fantasy|/genres/science-fiction|/genres/romance|/genres/adventure|\r\n",
    "```\r\n",
    "- Rating count, the number of people who have rated this book\r\n",
    "<br>\r\n",
    "<br> \r\n",
    "All this information can be seen on the web page.\r\n",
    "You will need to go to the developer tools and extract the necessary information. \r\n",
    "<br>\r\n",
    "<br>\r\n",
    "\r\n",
    "Since Published year and Genre require some extra processing to be extracted, we will start by writing 2 functions - `get_genre` and `get_year`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXJxwcREJGqQ"
   },
   "source": [
    "### 4.1 Extract Genres\r\n",
    "\r\n",
    "Write a function to get the genres which takes as input the HTML text and outputs a list of the genre URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2YIBd9H_38o"
   },
   "outputs": [],
   "source": [
    "#Extracting genre\n",
    "def get_genre(d):\n",
    "\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9WDGFGbJKzp"
   },
   "source": [
    "### 4.2 Extract Published Year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zP9BpEv5M2nk"
   },
   "source": [
    "Write a function to get the published year which takes as input the HTML text.\r\n",
    "\r\n",
    "You might have to use regular expressions to extract only the year Published Date seen in the web pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nM0Smiwf_38h"
   },
   "source": [
    "**Regular expressions** <br>\n",
    "Regular Expressions is a pattern matching mechanism used throughout Computer Science and programming (it's not just specific to Python). A tutorial on Regular Expressions (aka regex) is beond this lab, but below are many great resources that we recommend, if you are interested in them (could be very useful for a homework problem):<br>\n",
    "https://docs.python.org/3.3/library/re.html <br>\n",
    "https://regexone.com <br>\n",
    "https://docs.python.org/3/howto/regex.html <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U1rmGa9B_38j"
   },
   "outputs": [],
   "source": [
    "#Extracting published year\n",
    "yearre = r'\\d{4}'\n",
    "def get_year(d):\n",
    "  years=d.find(\"div\", attrs={\"class\": \"uitext darkGreyText\"})\n",
    "  years=years.findChildren(\"div\")[1].text\n",
    "  yearmatch=re.findall(yearre,years)\n",
    "  years_original=d.find_all(\"nobr\", attrs={\"class\": \"greyText\"})\n",
    "  if years_original!=[]:\n",
    "    finalyear=yearmatch[1]\n",
    "    return finalyear\n",
    "  else:\n",
    "      if len(yearmatch) > 0:\n",
    "          finalyear=yearmatch[0]\n",
    "      else:\n",
    "          finalyear=\"NA\"\n",
    "      return finalyear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-y8tarRJUJW"
   },
   "source": [
    "### 4.3 Extract Rating, ISBN, Title of the book, Author and Rating Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aemp8j2gJfNf"
   },
   "source": [
    "Now that you have created functions to extract genres and published years, you can extract the rest of the fields in a line or two.\r\n",
    "\r\n",
    "Extract the other fields and incorporate your functions to get a **list of dictionaries**. Each element in the list is a dictionary with the information you have extracted (Published year, Rating, ISBN, Title of the book, Author, Genres, Rating Count).\r\n",
    "<br>\r\n",
    "<br>\r\n",
    "So **each element in the list** should look something like this:\r\n",
    "```\r\n",
    "{'author': 'https://www.goodreads.com/author/show/153394.Suzanne_Collins',\r\n",
    " 'booktype': 'books.book',\r\n",
    " 'rating': 4.33,\r\n",
    " 'genres': '/genres/young-adult|/genres/fiction|/genres/science-fiction|/genres/dystopia|/genres/fantasy|/genres/science-fiction|/genres/romance|/genres/adventure|/genres/young-adult|/genres/teen|/genres/apocalyptic|/genres/post-apocalyptic|/genres/action',\r\n",
    " 'isbn': '9780439023481',\r\n",
    " 'ratingCount': '6554254',\r\n",
    " 'title': 'The Hunger Games (The Hunger Games, #1)',\r\n",
    " 'year': '2008'}\r\n",
    "```\r\n",
    "\r\n",
    "**Note**: Remember to convert your list of genres to a string seperated with the pipe character '|'.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5TKaGEt6_38p"
   },
   "outputs": [],
   "source": [
    "listofdicts=[]\n",
    "\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YKR5bCro_38y"
   },
   "outputs": [],
   "source": [
    "listofdicts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjtwDg2LJpuv"
   },
   "source": [
    "### 4.4 Creating a DataFrame\r\n",
    "\r\n",
    "Convert the list of dictionaries created above to a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0bGQ8l-_380"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(listofdicts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJPAaLqvQkuB"
   },
   "source": [
    "Convert this dataframe to a csv file and store it using `to_csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iSgShg17_383"
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"Goodreads.csv\", index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "distribute_Lab1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
